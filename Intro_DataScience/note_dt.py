'''ML: Classification (Decision Tree)
College of Global Business
Hoon Jang
[GLOB 347] Lecture note #7
Agenda
â€¢ What is Decision Tree?
â€¢ Learning Decision Trees
â€¢ Demo & Practice
2
Acknowledgements
All of the lecture notes for this class borrowed with/without modification from the sources
including lecture notes (prof. Tibshirani, Gâ€™Sell, Shalizi, Chouldechova, Franklin, etc.) and
books (Introduction to Business Analytics by Jaggia et al, etc.)
WHAT IS DECISION TREE?
Other Classification Techniques
â€¢ We learned about linear classification method (e.g., logistic
regression)
â€¢ Any other idea?
â€“ Pick an attribute, do a simple test
â€“ Conditioned on a choice, pick another attribute, do another test
â€“ In the leaves, assign a class with majority vote
â€“ Do other branches as well
â€¢ Reasonable?
4
Idea on Decision Tree
â€¢ Give axes aligned decision boundaries
5
Idea on Decision Tree (cont.)
6
â€¢ Here is a test example, then?
7cm
10 cm
Decision Trees
â€¢ A decision tree is a graphical depiction of a decision and
every potential outcome or result of making that decision
â€“ It gives people an effective and easy way to visualize and
understand the potential options of a decision and its range of
possible outcomes
â€¢ Properties
â€“ Branching is determined by attribute value
â€“ Leaf nodes are outputs (class assignment)
7
Decision Trees: Simple Rules
â€¢ Choose an attribute on which to descend at each level
â€¢ Condition on earlier (higher) choices
â€¢ Generally, restrict only one dimension at a time
â€¢ Declare an output value when you get to the bottom
â€¢ In the previous example, we only split each dimension once,
but that is not required
8
LEAERNING DECISION TREES
Learning Decision Trees
â€¢ Learning the simplest decision tree is â€œvery difficult problemâ€
â€“ If there are k features, a decision tree might have up to 2
k nodes
â€“ This is usually much too big in practice
â†’We want to find â€œefficientâ€ (smaller) trees
â€¢ Resort to a greedy heuristic:
â€“ Start from an empty decision tree
â€“ Split on next best attribute
â†’ Do this in a greedy manner by recursively choosing a best split
feature at each node
10
How to Choose a Good Attribute?
â€¢ Main Idea: A good feature splits the examples into subsets
that are (ideally) "all positive" or "all negativeâ€œ
â€“ All classes in leaf equally probable â†’ Bad!
â€¢ Technically, we will use Entropy!
11
Information Theory
â€¢ Entropy is the average level of information inherent in the
variableâ€™s possible outcomes
â€“ pi be the fraction of examples in class i
12
ğ¸ = âˆ’à·
ğ‘–=1
ğ‘š
ğ‘ğ‘–
log ğ‘ğ‘–
Information Gain
â€¢ Before split by f, the entropy is:
â€¢ After split by f, the entropy is:
â€“ Let ğ‘ğ‘–
ğ‘“
be the fraction of elements with feature f that lie in class i
â€“ Let ğ‘ğ‘–
âˆ’ğ‘“
be the fraction of elements without feature f that lie in class i
â€¢ The information gain = ğ¸ âˆ’ ğ¸ğ‘“ (Information = â€“entropy)
13
ğ¸ = âˆ’à·
ğ‘–=1
ğ‘š
ğ‘ğ‘–
log ğ‘ğ‘–
ğ¸ğ‘“ = âˆ’ğ‘
ğ‘“ à·
ğ‘–=1
ğ‘š
ğ‘ğ‘–
ğ‘“
log ğ‘ğ‘–
ğ‘“ âˆ’ ğ‘
âˆ’ğ‘“à·
ğ‘–=1
ğ‘š
ğ‘ğ‘–
âˆ’ğ‘“
log ğ‘ğ‘–
âˆ’ğ‘“
Example
â€¢ Using four features (â€œOutlookâ€, â€œ Temperatureâ€, â€œHumidityâ€,
â€œWindyâ€), estimate whether go play
â€“ Parentâ€™s entropy
14
Entropy (Play)
= (-5/14 log2
(5/14)) + (-9/14 log2
(9/14)
= 0.94
Example
â€¢ Using four features (â€œOutlookâ€, â€œ Temperatureâ€, â€œHumidityâ€,
â€œWindyâ€), estimate whether go play
â€“ Childrenâ€™s entropy
15
Entropy (Play, Outlook)
= P(sunny) * E(Play) + P(overcast) * E(Play) +
P(rainy) * E(play)
= 5/14 * [(-3/5 log2
(3/5)) + (-2/5 log2
(2/5)] +
4/14 * [(-4/4 log2
(4/4))] +
5/14 * [(-3/5 log2
(3/5)) + (-2/5 log2
(2/5)] +
= 0.69
Information gain = 0.94 â€“ 0.69 = 0.25
Entropy (Play, Outlook)
= P(true) * E(Play) + P(false) * E(Play)
= 0.89
Information gain = 0.94 â€“ 0.89 = 0.05
Choosing the Best Feature
â€¢ At each node, we choose the feature f which maximizes
the information gain
â€¢ This tends to be produced mixtures of classes at each node
that are more and more â€œpureâ€ as you go down the tree
â€¢ If a node has examples all of one class c, we make it a leaf
and output â€œcâ€. Otherwise, when we hit the depth limit, we
output the most popular class at that node
16
Constructing Decision Trees
â€¢ We made the fruit data partitioning just by eyeballing it
â€¢ Instead, we can use the â€œinformation gainâ€ to automate the
process. At each level, one must choose:
â€“ Which variable to split
â€“ Possible where to split it
â€¢ Choose them based on how much information we would
gain from the decision!
â€“ i.e., Choose attribute that gives the highest gain
17
Decision Tree Algorithm
â€¢ Simple, greedy, recursive approach, builds up tree node-bynode
â€“ Step 1: Pick an attribute to split at a non-terminal node
â€“ Step 2: Split examples into groups based on attribute value
â€“ Step 3: For each group:
â€¢ If no examples: Then, return majority from parent â†’ Stop
â€¢ Else if all examples in same class: Return class â†’ Stop
â€¢ Else Loop to Step 1
â€¢ Early stopping criteria
â€“ Restrict the tree depth to a certain level
â€“ Restrict the minimum number of observations allowed in any
terminal node
18
Decision Tree is Almighty?
â€¢ Though efficient, there are â€œMANYâ€ problems
â€“ You have exponentially less data at lower levels
â€“ Too big of a tree can overfit the data
â€“ Greedy algorithms donâ€™t necessarily yield the global optimum
â€¢ Any other alternatives?
19
DEMO & PRACTICE
Practice Problem
â€¢ Objective: Building a learning model on breast cancer data
â€¢ Breast cancer (BC)
â€“ One of the most common cancers among women worldwide
â€“ Early diagnosis of BC can improve the prognosis and chance of
survival significantly
â€“ Further accurate classification of benign tumors can prevent patients
undergoing unnecessary treatment
â†’ Classification and data mining methods are an effective way to
classify data
21
(FYI) Practice Problem (cont.)
â€¢ Recommended screening guidelines:
â€“ Mammography â€“ the most important screening test for breast
cancer is the mammogram. A mammogram is an X-ray of the breast.
It can detect breast cancer up to two years before the tumor can be
felt by you or your doctor
â€“ Women age 40-45 or older who are at average risk of breast
cancer should have a mammogram once a year
â€“ Women at high risk should have yearly mammograms along with
an MRI starting at age 30
22
Data
â€¢ Original data:
â€“ Use the UCI machine learning repository for breast cancer dataset (569 patients)
â€“ Is publicly available (was created by Dr.William H. Wolberg)
â€“ Dr. Wolberg uses â€œfluid sample, taken from patients with â€œsolid breast massesâ€ and
analyzed cytological features based on digital scan
â€¢ Attribute information
â€“ Diagnosis (M: Malignant; B: Benign) per each ID number
â€“ For each cell nucleus, 10 features are computed
â€¢ Radius
â€¢ Texture
â€¢ Perimeter
â€¢ Area
â€¢ Smoothness
â€¢ Compactness
â€¢ Concavity
â€¢ Concave points
â€¢ Symmetry
â€¢ Fractal dimension
â€“ Calculate mean, standard error, and worst or largest values
â†’ In total, for each patient, 30 features are calculated!!
23
Demo
â€¢ Questions:
â€“ Develop a decision tree model based on the data
â€“ Calculate four evaluation measures (accuracy, precision, recall, and F1-
score) of the model
â€“ Compare the evaluation measures with the ones from the logistic regression,
which one is better?
24'''